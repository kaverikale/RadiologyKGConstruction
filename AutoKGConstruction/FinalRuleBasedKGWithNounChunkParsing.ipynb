{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinalRuleBasedKGWithNounChunkParsing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjbEBYn5rP_5"
      },
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "import json\n",
        "\n",
        "#-------------------------------------\n",
        "# Read supersense dictionary\n",
        "#------------------------------------ \n",
        "\n",
        "supersense_dictionary = {}\n",
        "with open('SS_dictionary.json') as json_file:\n",
        "    supersense_dictionary = json.load(json_file)\n",
        "#print(supersense_dictionary)\n",
        "\n",
        "#-------------------------------------\n",
        "# Read ultrasound term dictionary\n",
        "#------------------------------------ \n",
        "\n",
        "rad_csv = pd.read_csv('radiology_dictionary.csv',names=['entity','category']) \n",
        "rad_dict = dict(zip(rad_csv.entity, rad_csv.category))\n",
        "\n",
        "\n",
        "#-------------------------------------\n",
        "# Read supersence-relations mapping\n",
        "#------------------------------------ \n",
        "\n",
        "rel_ss_df = pd.read_csv(\"SS_rel_mapping.csv\", names=['SS','rel'])\n",
        "\n",
        "dict_from_csv = dict(zip(rel_ss_df.SS, rel_ss_df.rel))\n",
        "\n",
        "#-------------------------------------\n",
        "# Read ultrasound term dictionary\n",
        "#------------------------------------ \n",
        "\n",
        "#dict_df = pd.read_csv('UltrasoundDictionary.csv',names=['entity']) \n",
        "dict_list = rad_csv['entity'].tolist()\n",
        "\n",
        "#dict_list = [_ for i in range(len(dict_list)) for _ in dict_list[i]]\n",
        "\n",
        "#print(dict_list[0:20])\n",
        "#--------------------------------------------------------------\n",
        "#  find max substring\n",
        "#--------------------------------------------------------------       \n",
        "def substringSieve(string_list):\n",
        "  out = []\n",
        "  for s in string_list:\n",
        "    if not any([s in entity for entity in string_list if s != entity]):\n",
        "      out.append(s)\n",
        "  return out\n",
        "\n",
        "def get_pos(word, tokens,pos_list):\n",
        "  try :\n",
        "    index = tokens.index(word)\n",
        "  except ValueError :\n",
        "    index  = 0\n",
        "  \n",
        "  return pos_list[index]\n",
        "\n",
        "\n",
        "def get_property_relation(ent_1,ent_2, single_entity):\n",
        "  \n",
        "  seq = 1\n",
        "  rel = 'PropertyOf'\n",
        "  try:\n",
        "    cat1 = rad_dict[ent_1]\n",
        "  except KeyError as e:\n",
        "    cat1 = 'property'\n",
        "  try:\n",
        "    cat2 = rad_dict[ent_2]\n",
        "  except KeyError as e:\n",
        "    cat2 = 'property'\n",
        "  if cat1 == 'anatomy' and cat2 == 'anatomy':\n",
        "    rel = 'PartOf'\n",
        "    #depend on adp : todo\n",
        "    if single_entity:\n",
        "      seq = 2\n",
        "    else:\n",
        "      seq = 1\n",
        "\n",
        "  elif cat1 == 'finding' and cat2 == 'anatomy':\n",
        "    rel = 'FoundIn'\n",
        "    seq = 1\n",
        "  elif cat1 == 'descriptor' and cat2 == 'anatomy':\n",
        "    rel = 'PropertyOf'\n",
        "    seq = 1\n",
        "  elif cat1 == 'property' and cat2 == 'finding':\n",
        "    rel = 'PropertyOf'\n",
        "    seq = 1\n",
        "  elif cat1 == 'property' and cat2 == 'anatomy':\n",
        "    rel = 'PropertyOf'\n",
        "    seq = 1\n",
        "  elif cat1 == 'descriptor' and cat2 == 'finding':\n",
        "    rel = 'PropertyOf'\n",
        "    seq = 1\n",
        "\n",
        "  elif cat2 == 'descriptor' and cat1 == 'anatomy':\n",
        "    rel = 'DescriptorOf'\n",
        "    seq = 2\n",
        "  elif cat2 == 'property' and cat1 == 'finding':\n",
        "    rel = 'PropertyOf'\n",
        "    seq = 2\n",
        "  elif cat2 == 'property' and cat1 == 'anatomy':\n",
        "    rel = 'PropertyOf'\n",
        "    seq = 2\n",
        "  elif cat2 == 'descriptor' and cat1 == 'finding':\n",
        "    rel = 'PropertyOf'\n",
        "    seq = 2\n",
        "\n",
        "\n",
        "\n",
        "  return rel, seq\n",
        "\n",
        "def process_noun_chunk(ent_1, token_SS_dict):\n",
        "  token1 = token_SS_dict[ent_1]\n",
        "  token_list = token1['token_list']\n",
        "  root_list = token1['root_list']\n",
        "  pos_list = token1['pos_list']\n",
        "  #print(token_list, root_list,pos_list )\n",
        "  #get root of element\n",
        "\n",
        "\n",
        "  dictinary_match_list  = [s for s in dict_list if (\" \" + s.strip() + \" \") in \" \" + ent_1.lemma_.strip() + \" \"]\n",
        "  dictinary_match_list = substringSieve(dictinary_match_list)   \n",
        "  #print(\"dictinary_match_list\",dictinary_match_list)\n",
        "\n",
        "    #find unmatched entities\n",
        "  unmatched_entities = []\n",
        "  if (len(dictinary_match_list)!=0):\n",
        "    matched_sent = ' '.join(dictinary_match_list)\n",
        "    unmatched_entities = set(token_list).difference(set(matched_sent.split()))\n",
        "\n",
        "      #unmatched_entities = unmatched_entities +  [item for item in token_list if item not in dict_item]\n",
        "  else:\n",
        "    unmatched_entities = token_list\n",
        "\n",
        "  #print('unmatched',unmatched_entities)\n",
        "  if (len(dictinary_match_list)!=0):\n",
        "    root_element_list = [s for s in dictinary_match_list if str(root_list) in s]\n",
        "    if (len(root_element_list) != 0):\n",
        "      root_element = root_element_list[0]\n",
        "    else:\n",
        "      root_element = str(root_list)        \n",
        "  else:\n",
        "    root_element = str(root_list)\n",
        "\n",
        "\n",
        "  for s in dictinary_match_list:\n",
        "      #print('s ',s)\n",
        "    if s != root_element:\n",
        "      s_pos = get_pos(s,token_list, pos_list )\n",
        "      if s_pos in (['PNOUN','NOUN','ADJ']):\n",
        "\n",
        "        rel, seq = get_property_relation(s, root_element, True)\n",
        "          \n",
        "        if seq == 1:\n",
        "          #writer.writerow({'ent1': s , 'rel' : rel ,'ent2': root_element })\n",
        "          print('ent1: ', s  , 'rel: ', rel ,'ent2: ',  root_element)\n",
        "        else:\n",
        "          #writer.writerow({'ent1': root_element , 'rel' : rel ,'ent2': s })\n",
        "          print('ent1: ', root_element  , 'rel: ', rel ,'ent2: ',  s)\n",
        "  \n",
        "  for word in unmatched_entities:\n",
        "    #print('word', word)\n",
        "    s_pos = get_pos(word,token_list, pos_list)\n",
        "    #print(type(word), type(root_element))\n",
        "    if word != root_element:\n",
        "      if s_pos in (['PNOUN','NOUN','ADJ']):\n",
        "        writer.writerow({'ent1': word , 'rel' : 'PropertyOf' ,'ent2': root_element })\n",
        "        print('ent1: ', word , 'rel: PropertyOf' ,'ent2: ',  root_element)\n",
        "\n",
        "  return root_element\n",
        "\n",
        "\n",
        "def get_relation(ent_1, verb, ent_2, token_SS_dict):\n",
        "  print('Input : ',ent_1, verb, ent_2)\n",
        "\n",
        "  #---------------------process entity 1\n",
        "\n",
        "  root_element1 = process_noun_chunk(ent_1, token_SS_dict)\n",
        "  root_element2 = process_noun_chunk(ent_2, token_SS_dict)\n",
        "\n",
        "  #----------------------------------\n",
        "\n",
        "  token1 = token_SS_dict[ent_1]\n",
        "  token1_list = token1['token_list']\n",
        "  root1_list = token1['root_list']\n",
        "  pos1_list = token1['pos_list']\n",
        "\n",
        "  token2 = token_SS_dict[ent_2]\n",
        "  token2_list = token2['token_list']\n",
        "  root2_list = token2['root_list']\n",
        "  pos2_list = token2['pos_list']\n",
        "  seq = 1\n",
        "  rel = 'PropertyOf'\n",
        "  if verb == None:\n",
        "    rel, seq = get_property_relation(root_element1, root_element2, False)\n",
        "    if seq == 1:\n",
        "      #writer.writerow({'ent1': s , 'rel' : rel ,'ent2': root_element })\n",
        "      print('ent1: ', root_element1  , 'rel: ', rel ,'ent2: ',  root_element2)\n",
        "    else:\n",
        "      #writer.writerow({'ent1': root_element , 'rel' : rel ,'ent2': s })\n",
        "      print('ent1: ', root_element2  , 'rel: ', rel ,'ent2: ',  root_element1)\n",
        "  \n",
        " \n",
        "  else:\n",
        "\n",
        "    try:\n",
        "      cat1 = rad_dict[root_element1]\n",
        "    except KeyError as e:\n",
        "      cat1 = 'NotFound'\n",
        "    try:\n",
        "      cat2 = rad_dict[root_element2]\n",
        "    except KeyError as e:\n",
        "      cat2 = 'NotFound'\n",
        "\n",
        "    if cat1 == 'NotFound' or cat2 == 'NotFound':\n",
        "      varb_token = token_SS_dict[verb]\n",
        "      verb_SS_list = varb_token['ss_list']\n",
        "      ss = verb_SS_list[0]\n",
        "      rel = dict_from_csv[ss]\n",
        "      print('ent1:', root_element1, 'rel:', rel, 'ent2:', root_element2)\n",
        "\n",
        "    else:\n",
        "\n",
        "      if cat1 == 'anatomy' and cat2 == 'anatomy':\n",
        "        rel = 'PartOf'\n",
        "        #depend on adp : todo\n",
        "        seq = 1\n",
        "      elif cat1 == 'finding' and cat2 == 'anatomy':\n",
        "        rel = 'FoundIn'\n",
        "        seq = 1\n",
        "      elif cat1 == 'descriptor' and cat2 == 'anatomy':\n",
        "        rel = 'PropertyOf'\n",
        "        seq = 1\n",
        "      elif cat1 == 'property' and cat2 == 'finding':\n",
        "        rel = 'PropertyOf'\n",
        "        seq = 1\n",
        "      elif cat1 == 'property' and cat2 == 'anatomy':\n",
        "        rel = 'PropertyOf'\n",
        "        seq = 1\n",
        "\n",
        "      elif cat2 == 'descriptor' and cat1 == 'anatomy':\n",
        "        rel = 'DescriptorOf'\n",
        "        seq = 2\n",
        "      elif cat2 == 'property' and cat1 == 'finding':\n",
        "        rel = 'FoundIn'\n",
        "        seq = 1\n",
        "      elif cat2 == 'property' and cat1 == 'anatomy':\n",
        "        rel = 'PropertyOf'\n",
        "        seq = 2\n",
        "      \n",
        "      if seq == 1:\n",
        "        print('ent1:', root_element1, 'rel:', rel, 'ent2:', root_element2)\n",
        "      else:\n",
        "        print('ent1:', root_element2, 'rel:', rel, 'ent2:', root_element1)\n",
        "\n",
        "  return rel, seq"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2fdC_a1WkLk"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZ7VOLP0xpXh",
        "outputId": "f56319f6-85e8-476c-c37d-63aa540599f8"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "#nlp.add_pipe(\"merge_entities\")\n",
        "\n",
        "#-------------------------------------\n",
        "# Read cleaned text data\n",
        "#------------------------------------ \n",
        "\n",
        "df = open('Data.txt','r') \n",
        "lines = df.readlines()\n",
        "\n",
        "debugFile = open('Debug.txt', 'w')\n",
        "\n",
        "#-------------------------------------\n",
        "# Open file to write triplets\n",
        "#------------------------------------ \n",
        "\n",
        "csvfile = open('NewKG.csv', 'w', newline='\\n')\n",
        "fieldnames = ['ent1', 'rel', 'ent2']\n",
        "writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "writer.writeheader()\n",
        "\n",
        "#-------------------------------------\n",
        "# For each line find chunk dependencies\n",
        "#------------------------------------ \n",
        "\n",
        "error_cnt = 0\n",
        "for line in lines:\n",
        "  if line.find('/') != -1:\n",
        "    continue\n",
        "  print(line)\n",
        "  sentence_matadata_df = pd.DataFrame(columns=['chunk_token','chunk_POS','chunk_dep','token_list','index_list','ss_list','root_list'])\n",
        "  ent1 = line.replace('\\n','')\n",
        "  ent1 = ent1.lower()\n",
        "  \n",
        "  #-------------------------------------\n",
        "  #get supersenses\n",
        "  #-------------------------------------\n",
        "  try:\n",
        "    value1 = supersense_dictionary[ent1]\n",
        "  except KeyError:\n",
        "    error_cnt = error_cnt + 1\n",
        "    #print(error_cnt)\n",
        "    continue\n",
        "  ent1_val = pd.DataFrame(value1).T\n",
        "  ent1_val.columns = ['token', 'pos', 'supersense']  \n",
        "  supersense = ent1_val['supersense'].tolist()\n",
        "  pos_list = ent1_val['pos'].tolist()\n",
        "  #print(supersense)\n",
        "  doc = nlp(ent1)\n",
        "  root_list = []\n",
        "  for chunk in doc.noun_chunks:\n",
        "    if (len(chunk)>1):\n",
        "      root_list.append(chunk.root.lemma_)\n",
        "  \n",
        "  nlp.add_pipe(nlp.create_pipe('merge_noun_chunks'))\n",
        "  doc = nlp(ent1)\n",
        "  \n",
        "  #for token in doc:\n",
        "  #    print(token.text, token.pos_, token.dep_)\n",
        "  sentences = list(doc.sents)\n",
        "  sent = sentences[0]\n",
        "\n",
        "  leaf_nodes = []\n",
        "\n",
        "  root_token = sent.root\n",
        "  #get all leaf nodes\n",
        "  final_root_list = []\n",
        "  cnt = 0\n",
        "  i = 0\n",
        "  token_SS_dict = dict()\n",
        "\n",
        "  for token in sent:   \n",
        "\n",
        "    if (token.n_lefts + token.n_rights) == 0 :\n",
        "      leaf_nodes.append(token)\n",
        "\n",
        "    tokens= token.lemma_.split(' ')\n",
        "    if (len(tokens)>1):\n",
        "      root = root_list[cnt]\n",
        "      cnt = cnt + 1\n",
        "    else:\n",
        "      root = token.lemma_\n",
        "    index = [item for item in range(i, i + len(tokens))]\n",
        "\n",
        "    ss_list = supersense[i:i + len(tokens)]\n",
        "    pos_lst = pos_list[i:i + len(tokens)]\n",
        "    df2 = {'token_list':tokens, 'index_list':index, 'pos_list':pos_lst, 'ss_list':ss_list, 'root_list':root}\n",
        "    \n",
        "    token_SS_dict[token] = df2\n",
        "\n",
        "    i = i + len(tokens)\n",
        "\n",
        "  #print('token_SS_dict',token_SS_dict)\n",
        "  #print(sent, leaf_nodes)\n",
        "  print(leaf_nodes)\n",
        "  \n",
        "  object_to_process = []\n",
        "  verb_to_process = []\n",
        "  adp_to_process = []\n",
        "  subj_list = []\n",
        "  path_aux = []\n",
        "\n",
        "  #foreach leaf node find path upto root\n",
        "\n",
        "  for leaf_token in leaf_nodes:\n",
        "    path_obj = []\n",
        "    path_adp = []\n",
        "    path_subj = []\n",
        "    temp_path_subj = []\n",
        "    token = leaf_token\n",
        "    #print('leaf', leaf_token)\n",
        "    while True:\n",
        "      #find ancestor\n",
        "      #print('token',token)\n",
        "      print(token)\n",
        "      if token.dep_ == 'attr' and token.pos_ == 'NOUN' or token.pos_ == 'PROPN':\n",
        "        path_obj.append(token)\n",
        "      if token.dep_ == 'appos' and token.pos_ == 'NOUN' or token.pos_ == 'PROPN':\n",
        "        if len(path_obj) != 0:\n",
        "          prev_obj = path_obj.pop()\n",
        "          if len(path_adp)!=0:\n",
        "            prev_adp = path_adp.pop()\n",
        "            rel, seq = get_relation( prev_obj, prev_adp, token,token_SS_dict)\n",
        "\n",
        "      if token.dep_ == 'xcomp':\n",
        "        verb_to_process.append(token)\n",
        "\n",
        "      if token.pos_ == 'AUX':\n",
        "        path_aux.append(token)\n",
        "\n",
        "      if token.dep_ == 'prep':\n",
        "        path_adp.append(token)\n",
        "\n",
        "      if token.dep_ == 'nsubj' or token.dep_ == 'nsubjpass':        \n",
        "        subj_list.append(token)\n",
        "        subj_list = subj_list + temp_path_subj\n",
        "        #remove from obj\n",
        "        #print('before obj', object_to_process,temp_path_subj)\n",
        "        object_to_process = [i for i in object_to_process if i not in temp_path_subj]\n",
        "        #print('after obj', object_to_process)\n",
        "        if len(path_obj) != 0:\n",
        "          prev_obj = path_obj.pop()\n",
        "          if len(path_adp)!=0:\n",
        "            prev_adp = path_adp.pop()\n",
        "            rel, seq = get_relation( prev_obj, prev_adp, token,token_SS_dict)\n",
        "\n",
        "     \n",
        "      #-----------------------process conj\n",
        "      \n",
        "      if token.dep_ == 'conj':\n",
        "\n",
        "        if token.pos_ == 'NOUN' or token.pos_ == 'PROPN':\n",
        "          if len(path_obj) != 0:\n",
        "            prev_obj = path_obj.pop()\n",
        "            if len(path_adp)!=0:\n",
        "              prev_adp = path_adp.pop()\n",
        "              rel, seq = get_relation( prev_obj, prev_adp, token,token_SS_dict)\n",
        "        else:\n",
        "          verb_processed = False\n",
        "          right_childeren = token.rights\n",
        "          child = None\n",
        "          for r_child in right_childeren:\n",
        "            child = r_child\n",
        "            break\n",
        "          if child != None:\n",
        "            if child.dep_ == 'pobj' or child.dep_ == 'dobj':\n",
        "              verb_processed = True\n",
        "              rel, seq = get_relation( token, None, child, token_SS_dict)\n",
        "              #print('token',token)\n",
        "\n",
        "          if token.pos_ == 'VERB' and not verb_processed:\n",
        "            verb_to_process.append(token)\n",
        "          elif not verb_processed:\n",
        "            object_to_process.append(token)\n",
        "            temp_path_subj.append(token)\n",
        "            #print('temp_path_subj',token,temp_path_subj)\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "      if token.dep_ == 'pobj' or token.dep_ == 'dobj':\n",
        "\n",
        "        if len(path_obj) != 0:\n",
        "          prev_obj = path_obj.pop()\n",
        "          if len(path_adp)!=0:\n",
        "            prev_adp = path_adp.pop()\n",
        "            rel, seq = get_relation( token, prev_adp, prev_obj,token_SS_dict)\n",
        "\n",
        "        path_obj.append(token)\n",
        "\n",
        "      if token.pos_ == 'ADJ':\n",
        "        #print('ADJ', token, len(path_obj), len(path_adp))\n",
        "        if len(path_obj) != 0:\n",
        "          prev_obj = path_obj[0]\n",
        "          if len(path_adp)!=0:\n",
        "            prev_adp = path_adp.pop()\n",
        "            rel, seq = get_relation( token, prev_adp, prev_obj,token_SS_dict)\n",
        "\n",
        "      #print('token',token)\n",
        "      \n",
        "      for ancestor in token.ancestors:\n",
        "        ans = ancestor\n",
        "        #print('ans',ans)\n",
        "        break\n",
        "\n",
        "      if token.pos_ == 'AUX' and ans.pos_ == 'VERB':\n",
        "        verb_to_process.append(ans)\n",
        "\n",
        "      if (ans == root_token):\n",
        "        if token.pos_ == 'VERB':\n",
        "          print(token)\n",
        "          verb_to_process.append(token)\n",
        "          \n",
        "        if ans.dep_ == 'ROOT' and (ans.pos_ == 'NOUN' or ans.pos_ == 'PROPN'):\n",
        "          #print(\"In root\")\n",
        "          if len(path_obj) != 0:\n",
        "            prev_obj = path_obj.pop()\n",
        "            if len(path_adp)!=0:\n",
        "              prev_adp = path_adp.pop()\n",
        "              rel, seq = get_relation( ans, prev_adp, prev_obj,token_SS_dict)\n",
        "              #print('ent1:', ans.text, 'rel:', prev_adp.text, 'ent2:', prev_obj.text)\n",
        "        object_to_process = object_to_process + path_obj\n",
        "        adp_to_process = adp_to_process + path_adp\n",
        "        \n",
        "        break\n",
        "       # if is_last_obj_processed:\n",
        "        #  break\n",
        "        #else:\n",
        "          # find subject of root\n",
        "         # object_to_process.append()\n",
        "          #findSubject()\n",
        "      else:\n",
        "        #print('ans',ans)\n",
        "        token = ans\n",
        "\n",
        "  #print(subj_list,object_to_process, verb_to_process)\n",
        "\n",
        "  subj_list = set(subj_list)\n",
        "  object_to_process = set(object_to_process)\n",
        "  verb_to_process = set(verb_to_process)\n",
        "\n",
        "  #if len(subj_list)!=0 and len(object_to_process)==0 and len(verb_to_process)==0:\n",
        "\n",
        "\n",
        "\n",
        "  if len(subj_list)!=0 and len(object_to_process)!=0:\n",
        "    for sub in subj_list:\n",
        "      for obj in object_to_process:\n",
        "        if len(adp_to_process) != 0:\n",
        "          rel = adp_to_process.pop()\n",
        "        else:\n",
        "          rel = root_token\n",
        "        rel, seq = get_relation( sub, rel, obj,token_SS_dict)\n",
        "\n",
        "  if len(subj_list) and len(verb_to_process)!=0:\n",
        "    for sub in subj_list:\n",
        "      for obj in verb_to_process:\n",
        "        rel, seq = get_relation( obj, None, sub,token_SS_dict)\n",
        "\n",
        "  nlp.remove_pipe('merge_noun_chunks')\n",
        "  \n"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "small calculus seen in middle 1.1 mm and lower 11 mm calyceal group of right kidney.\n",
            "[middle, 1.1 mm, and, right kidney, .]\n",
            "middle\n",
            "in\n",
            "seen\n",
            "seen\n",
            "Input :  small calculus in middle\n",
            "ent1:  small rel:  PropertyOf ent2:  calculus\n",
            "ent1:  small rel:  PropertyOf ent2:  calculus\n",
            "ent1: calculus rel: FoundIn ent2: middle\n",
            "1.1 mm\n",
            "seen\n",
            "seen\n",
            "and\n",
            "seen\n",
            "seen\n",
            "right kidney\n",
            "of\n",
            "11 mm calyceal group\n",
            "Input :  11 mm calyceal group of right kidney\n",
            "ent1:  calyceal rel: PropertyOf ent2:  group\n",
            "ent1: group rel: PartOf ent2: right kidney\n",
            "lower\n",
            "Input :  lower None 11 mm calyceal group\n",
            "ent1:  calyceal rel: PropertyOf ent2:  group\n",
            "ent1:  low rel:  PropertyOf ent2:  group\n",
            "seen\n",
            "seen\n",
            ".\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}